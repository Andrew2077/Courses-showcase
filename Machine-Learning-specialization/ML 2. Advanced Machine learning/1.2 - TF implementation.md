## Model training steps 

1- Define the model function given 
 -  input x
 -  parameters x, b

$$
F_{w,b}(X) = ?? -> Logistic Regresssion
$$

2- Define loss cost functions

$$
L(F_{w,b}(x), y)
$$
$$
J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(F_{w,b}(x^{(i)}), y^{(i)})
$$

3- Train the model to minimize J

$$
w = w - \alpha * \frac{\partial J}{\partial w}
$$


$$
b = b - \alpha * \frac{\partial J}{\partial b}
$$

## Implementation with TF
1- Creating model

```python
import tensorflow as tf 
from keras import Sequential , layers

#* creating a model with tf
model = Sequential([
    layers.Dense(unites = 25, activation = 'sigmoid'),
    layers.Dense(unites = 1, activation = 'sigmoid')
])

```

2- defining the model loss and cost functions

- mode.compile 

```python 
from keras.losses import BinaryCrossentropy, MeanSquareError()

model.compile(
	loss = BinaryCrossentropy() 
	#or 
	loss =MeanSquareError()  
)

```

3- updating Gradient Descent

```python 
model.fit(X,Y, epochs=100 )
```



4- Choosing the right Activation function 

- Sigmoid 
- Relu 
- Linear - no activation 

In Hidden layers use **Relu** as the default 
In the Output layer it depends on the problem and what you are trying to predict 