[TOC]
# 1. Neural Network - Deep Learning
## 1.1 Intuitions 
### why Deep learning ?
- because the algorithms like logistic regression, regression etc. couldn't keep up with the increasing amount of data 
- while a small neural network performed much better than most of these basic algorithms
- advancing GPU allowed for that 

## 1.2 Demand prediction - *has to be written*
- logistic regression is a very simplified model of a single Neuron 
- what the neuron does is
  - takes an x ( **price** ) and outputs a ( **probability** )
 - NN is a number of Neurons connected together aka a layer 
 - a Layer can have multiple neurons or one single layer 
 - output layer that has 1 neuron that gives the output
 - A neural network is a combination of layers or Neurons 
 - Layers functions ( **awareness** or **affordability** ) are called **Activations** 

- the layer example is as follows 

	-  input layer,  Vector 4 inputs ,  -->> hidden layer, Vector 3 inputs -->> output layer, 1 output

	-  each neuron in the input layer is connected to all the inputs 
	
	-  you think about it as a logistic regression that builds its own features
	
	-  the NN figures out its own functions, you don't have to specify it 
	
	-  it can have more than 1 hidden layer, multi-layer perception
	
	- the number of hidden layers is an architectural thing, and impacts the performance of the NN 

## 1.3 eg. Facial recognition

- detect edges from the matrix-vector of the image - 1st layer
- form feature - 2nd layer
- form faces - 3rd layer

## 1.4 layers 
each neuron performs a Sigmoid function on the Input and spits an output 

| Input      | layers    | output |
| :--------- | --------- | ----------- |
| X - vector | w_1 , b_1 | g(x . W1 +b1) |
| X- vector  |w_2 , b_2|  g(x . W2 +b2)|
| X- vector |w_3 , b_3 |  g(x . W3 +b3)           |

Happens on all Convolution Neural Network layers 

The output of layer is an input for another 

on the value of the OUTPUT layer we can predict category either 0 or 1 depending on the output layer 

## 1.5 Notation

$$
a_J^{[L]} = g(w_J^{[L]}  . a^{[L-1]} + b_J^{[L]})
$$

where 
 - a: is the output 
 - g: is the sigmoid function or Activation function 
 - L: layer number 
 - J: Neuron number 
 - W: the Neuron weights 
 - b: Neuron constant - bias

 at L = 1, $$ a^{[L-1]}  = X$$ , the Input to layer 1 