## Working with keras

```python
import tensorflow as tf
from keras.layers import Dense, Input #* Dense is a layer, Input is a tensor 
from keras import Sequential #* Sequential is a model
```

Dense is a type of layer  - where each node in the layer is connected to all nodes in the following layer
Sequential is a type of model - where each layer is read sequentially 

## creating layers 

```python
linear_layer = tf.keras.layers.Dense(units=1, activation = 'linear', )

#or 

from tf.keras.layers import Dense
linear_layer = Dense(units=1, activation = 'linear', )  # if imported directly
```

## Creating models

```python
model = Sequential(
    [
        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')
    ]
)

model.summary() # shows the number of layers imprtant for showing your model
```

- Unites: number of neurons 
- input_dim: the dimension of the layer input
- activation: activation function , 'sigmoid' or 'linear'
- name: layer name

## model parameters and predictions

- they are initialized randomly for now
- output of the layer is printed as follows 
```python 
a = layer(xtrain.respahe(-1,1)) # depends on the dimension of Layer input_dim
layer.get_weights()
layer.set_weights()
a = (xtrain) # print the new layer values
model.predict(xtrain)
```

keep in mind that when initializing Vectors from NumPy u have to initialize it this way to have the dimension of (1,n)

tensor is an array of dimensions (1, n), Tensor is a Metrix or a way to represent a Metrix  

> "but when you pass a NumPy inside internal flow, TensorFlow likes to convert it to its inside internal format. The tensor then operates efficiently using tensors. And when you read the data back out you can keep it as a tensor or convert it back to a NumPy array" ~~ Andrew Ng

```python
vector = np.array([[1,2,3,4]]) # [1 2 3 4]

a.numpy() #will turn tensor into a np array
```

## the process of creating a NN model 
```python 
from keras.layers import Dense
from keras.models import Sequential

#creating model
layer1 =Dense(unites = 3, activation = 'sigmoid')
layer2 =Dense(unites = 1, activation = 'sigmoid')

#forward propagation
model = Sequential([layer1 , layer2])# better to build layers insdide model itself , better performance.

#compiling 
model.compile() #more on that later 
model.fit(x,y)

#inference 
model.predict(x_new)
```

### Normalize Data
Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. 
The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:

- create a "Normalization Layer". Note that this is not a layer in your model 
- `adapt` the data. This learns the mean and variance of the data set and saves the values internally.
- normalize the data.  
It is important to apply normalization to any future data that utilizes the learned model.

```python
norm_l = tf.keras.layers.Normalization(axis=-1) # create a normalization layer
norm_l.adapt(X)  # learns mean, variance
Xn = norm_l(X) # normalize X
```

This is a way to increase the training data 

```python
Xt = np.tile(Xn,(1000,1)) #xtrain 
Yt= np.tile(Y,(1000,1))    #ytain 
print(Xt.shape, Yt.shape)  
```

Creating the model 

```python
tf.random.set_seed(1234)  # setting random seed

model = Sequential(
    [
        #* intialize methode 1 (default), specify input shape for all layers
        tf.keras.Input(shape=(2,)),
        Dense(3, activation='sigmoid', name = 'layer1' ,),
        Dense(1, activation='sigmoid', name = 'layer2', ),
        
        #* intialize methode 2, specify the input shape of layers
         Dense(3, activation='sigmoid', name = 'layer1' , input_dim = 2),
         Dense(1, activation='sigmoid', name = 'layer2', input_dim = 2),
     ]
)

model.summary()
```
## some methods will be known later 

- The `model.compile` statement defines a loss function and specifies a compile optimization.
- The `model.fit` statement runs gradient descent and fits the weights to the data.

```python
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
)

model.fit(Xt, Yt, epochs=10,)
```

