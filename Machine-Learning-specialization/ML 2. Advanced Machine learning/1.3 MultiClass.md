## MultiClass classification

### Softmax 
class function, sigmoid z
$$
z_j  = w_j * x + b_j
$$
output function a
$$
a_j = \frac{e^{z_j}}{\sum_{i}^N{e^{z_j}}}
$$

```python
import tensorflow as tf
from keras import Sequential


model = Sequential([
    	layers.Dense(25, activation = 'relu'),
        layers.Dense(12, activation = 'relu', kernel_regulaizer.l2(lambda))#* regulairzation paramerter
        layers.Dense(5, activation = 'softmax')#* use linear with From_logits = True
])

#* when compiling choose other loss function 
from keras.losses import SparseCategoricalCrossentropy, 
model.compile(
		loss = SparseCategoricalCrossentropy() #this will result in a round off error
)

#* better to implement it that way 
model.compile(
		loss = SparseCategoricalCrossentropy(from_logits = True), #add flexibilty to TF 
    	optimizer = tf.keras.optimizers.Adam(lr= 0.001)#* adjustable learning rate
)

model.fit(x, y , epochs = 100)

#* Prediction it's different when using logits 

logits = model(X) #* z1 to zn 
f_x = tf.nn.softmax(logits) #* a1 to a10 

#when applying logits tf returns the z function not a function
```